{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mnijhuis-dnb/Artificial_Intelligence_and_Machine_Learning_for_SupTech/blob/main/Tutorials/Tutorial%207%20NLTK%20and%20sentiment%20analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Artificial Intelligence and Machine Learning for SupTech  \n",
        "Tutorial 7: NLTK and sentiment analysis\n",
        "\n",
        "*\tConstructing a bag of words\n",
        "*\tClassifying sentiments (positive/negative)\n",
        "*\tExample with financial news data\n",
        "\n",
        "<br/>\n",
        "\n",
        "15 March 2023  \n",
        "\n",
        "**Instructors**  \n",
        "Prof. Iman van Lelyveld (iman.van.lelyveld@vu.nl)<br/>\n",
        "Dr. Michiel Nijhuis (m.nijhuis@dnb.nl)  "
      ],
      "metadata": {
        "id": "-x9uRovz0R8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation"
      ],
      "metadata": {
        "id": "A1b1zu7x3drW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re"
      ],
      "metadata": {
        "id": "nAUm1PTv3Y5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('vader_lexicon')\n",
        "!pip install twython "
      ],
      "metadata": {
        "id": "d4CjVU8TPOWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load articles from the FT\n",
        "The file `data/df_articles.csv` contains 436 articles from the Financial Times on the topic of Environmental, Social and Governance (ESG) between March 2021 and 2022."
      ],
      "metadata": {
        "id": "frYw4kD24Jyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1-4lW68b60PikNWQOfqc5qtriOEHC3_Kq"
      ],
      "metadata": {
        "id": "MOEtgav4_-bS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_articles = pd.read_csv('/content/df_articles.csv')\n",
        "df_articles"
      ],
      "metadata": {
        "id": "kC7TNg_V4Jyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract tokens from the text\n",
        "Current, the article is stored as `date`, `title` and `body`. Our focus will be on `body`. This is currently stored as a long string of text. See example below"
      ],
      "metadata": {
        "id": "aW_ZrPJI4Jyd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_articles.loc[10, 'title']"
      ],
      "metadata": {
        "id": "3v-xDoJiKKiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_articles.loc[10, 'body']"
      ],
      "metadata": {
        "id": "c5sQTRWd4Jyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is not particularly useful as we want to extract meaningful words. For example, \"green\" or \"inflation\" or \"rise\". For this we first have to splice the `body` into individual tokens, that are separated by whitespaces. The regular expression `'\\w+'` means one or more alphanumeric characters. "
      ],
      "metadata": {
        "id": "SBZO9zSe4Jyd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "# \\w+ means at least one white space or line break character\n",
        "regexp = RegexpTokenizer('\\w+')"
      ],
      "metadata": {
        "id": "ZqJp_KMP4Jyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_articles['body_tokenize'] = df_articles['body'].apply(regexp.tokenize)"
      ],
      "metadata": {
        "id": "EjLNfKpK4Jyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_articles.loc[0, 'body']"
      ],
      "metadata": {
        "id": "aGLnrmYrLg0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_articles.loc[0, 'body_tokenize']"
      ],
      "metadata": {
        "id": "6nwfu12jdg7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert all words to lower case"
      ],
      "metadata": {
        "id": "-HdKZbaa6G7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_lowercase(list_of_tokens):\n",
        "  return [tk.lower() for tk in list_of_tokens]"
      ],
      "metadata": {
        "id": "GWK0EGO76Nb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convert_to_lowercase(['BaBaBa','bbbb','AAAA'])"
      ],
      "metadata": {
        "id": "JNVFvuLjd8oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_articles['body_tokenize'] = df_articles['body_tokenize'].apply(convert_to_lowercase)"
      ],
      "metadata": {
        "id": "llJsqUkz6J-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_articles.loc[10, 'body_tokenize']"
      ],
      "metadata": {
        "id": "8jZ7vl5k6wbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove stopwords and short words\n",
        "This is much better. But many words are not informative, which are what we call \"stopwords\"."
      ],
      "metadata": {
        "id": "HwAELygp4Jye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
        "stopwords"
      ],
      "metadata": {
        "id": "-64eOIwn4Jye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to filter out these words and only keep the meaningful ones. Moreover, short words that are one or two characters long can also be removed."
      ],
      "metadata": {
        "id": "RW4g8XfR4Jye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(tokens):\n",
        "    non_stopwords_tokens = []\n",
        "    for tk in tokens:\n",
        "        if len(tk) <= 3:\n",
        "            # if the token is less than 3 characters long, jump to next token\n",
        "            continue\n",
        "\n",
        "        if any([a.isnumeric() for a in tk]):\n",
        "            # if the token is numeric, e.g. '34' yields true\n",
        "            continue\n",
        "\n",
        "        if tk in stopwords:\n",
        "            # if the token is a stopword, jump to next token\n",
        "            continue\n",
        "\n",
        "        # if no jumps happened, then add the token to the results\n",
        "        non_stopwords_tokens.append(tk)\n",
        "\n",
        "    return non_stopwords_tokens"
      ],
      "metadata": {
        "id": "JmY-cMPW4Jye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remove_stopwords([\n",
        "    'ab',\n",
        "    'abc',\n",
        "    'only',\n",
        "    '2020a',\n",
        "    'England',\n",
        "    'ESG',\n",
        "])"
      ],
      "metadata": {
        "id": "JtZDEhO9egjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_articles['body_tokenize_nonstop'] = df_articles['body_tokenize'].apply(remove_stopwords)"
      ],
      "metadata": {
        "id": "HsTQgI3m4Jye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_articles.loc[10, 'body_tokenize_nonstop']"
      ],
      "metadata": {
        "id": "Dv4pwzYZ4Jye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming and lemmatization\n",
        "\n",
        "Great, now we need to use stemming or lemmatization to identify inflected forms of a word. I.e. `look` is the base form, but we usually see words like `looking`, `looks`, `looked` etc. Lemmatization goes a step further and also takes the context into account."
      ],
      "metadata": {
        "id": "Nx6RfcTB4Jye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer().lemmatize"
      ],
      "metadata": {
        "id": "-AZbA_Of4Jye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer('bikes')"
      ],
      "metadata": {
        "id": "TrdjVa6G4Jye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer('going')"
      ],
      "metadata": {
        "id": "Ss_cXFc54Jye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_each_word(words):\n",
        "    lemmatized_words = []\n",
        "    for word in words:\n",
        "        lemmatized = lemmatizer(word)\n",
        "        lemmatized_words.append(lemmatized)\n",
        "    return lemmatized_words"
      ],
      "metadata": {
        "id": "XVoN7Lz04Jyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatize_each_word(df_articles.loc[10, 'body_tokenize_nonstop'])"
      ],
      "metadata": {
        "id": "d8JzN6FE4Jyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_articles['body_tokenize_nonstop_lemma'] = df_articles['body_tokenize_nonstop'].apply(lemmatize_each_word)"
      ],
      "metadata": {
        "id": "_6oT2nZP4Jyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assign sentiments "
      ],
      "metadata": {
        "id": "i0OIEJ7X4wwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "analyzer = SentimentIntensityAnalyzer().polarity_scores"
      ],
      "metadata": {
        "id": "J1DR_-JE4Jyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyzer('good')"
      ],
      "metadata": {
        "id": "neU1Sqb3GywW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyzer('bad')"
      ],
      "metadata": {
        "id": "jWGLCgstG7qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyzer('I like coffee')"
      ],
      "metadata": {
        "id": "eG3OPSSeG9CV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentiment(list_of_words):\n",
        "    words = ' '.join(list_of_words)\n",
        "    analyzed = analyzer(words)\n",
        "    return analyzed['compound']"
      ],
      "metadata": {
        "id": "WDzCXDiK4Jyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_sentiment(['I', 'like', 'coffee'])"
      ],
      "metadata": {
        "id": "3QjRKEV6HBvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_articles['sentiment'] = df_articles['body_tokenize_nonstop_lemma'].apply(get_sentiment)"
      ],
      "metadata": {
        "id": "uO9Vixes4Jyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_articles['sentiment']"
      ],
      "metadata": {
        "id": "2PQfCiqvHNkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot results"
      ],
      "metadata": {
        "id": "zvFBz2yGGceo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_articles.head()"
      ],
      "metadata": {
        "id": "j5qQcMFXHSoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have the desired results. We would like to plot these over time. So let's extract the `sentiment` column and plot it."
      ],
      "metadata": {
        "id": "ysVYsQtsHdoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_articles['sentiment'].plot(figsize=[15,5])"
      ],
      "metadata": {
        "id": "q6_pcyMPHkmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is very messy. There are several issues here that need to be addressed\n",
        "1. The x-axis is wrong, it should be based on dates, not number of entries.\n",
        "2. The series is very volatile, we are looking for a trend.\n",
        "\n",
        "Let us first make our problem easier. `df_articles` contains all the data we constructed throughout the notebook. For plotting, however, we only need two columns: (a) the `date` column as the index and (b) the `sentiment` column for the actual time series values."
      ],
      "metadata": {
        "id": "hqKvVhx8Ho1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_articles.head(5)"
      ],
      "metadata": {
        "id": "hBhq4aHrIS6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting the dates right"
      ],
      "metadata": {
        "id": "mN-nAbqhMXPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sr_sentiment = df_articles.set_index('date')\n",
        "sr_sentiment = sr_sentiment['sentiment']"
      ],
      "metadata": {
        "id": "jSfarYwgI8At"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sr_sentiment.plot(figsize=[15,5], marker='o')"
      ],
      "metadata": {
        "id": "M0NUpl_ZJFWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This looks identical as the previous figure, only that the x-labels are different. If we look closely, however, the dates are not actual dates. Instead, these are texts. Let's have a look at the index of `sr_sentiment`"
      ],
      "metadata": {
        "id": "YRy-9bWIJ2_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sr_sentiment.index"
      ],
      "metadata": {
        "id": "_zKxwGIVKIF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `dtype='object'` means that the index is just a list of string values. We want to have dates."
      ],
      "metadata": {
        "id": "rwxNln4zKKiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sr_sentiment.index = pd.to_datetime(sr_sentiment.index)\n",
        "sr_sentiment.index"
      ],
      "metadata": {
        "id": "UUF0pr-pKRGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sr_sentiment.plot(figsize=[15,5], marker='o', lw=0)"
      ],
      "metadata": {
        "id": "zGIxgjEgKXWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ensuring a consistent timeseries"
      ],
      "metadata": {
        "id": "MibpSBclMbxm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What's different? A lot of diagonal lines appear that seem to jump between data points. This is usually a sign of interpolation and missing values. \n",
        "\n",
        "Can we see the missing values? Let's have look at `sr_sentiment`. Notice anytihng strange?"
      ],
      "metadata": {
        "id": "9D3K6VPbJUnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sr_sentiment"
      ],
      "metadata": {
        "id": "65nexg8NKkj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indeed! There are two data points for the `2022-02-26`. We should only have on value for each day. Also, ther edoes not seem to be a value for `2022-02-05`.\n",
        "\n",
        "Thanks to the index being in date format, we can make use of pandas' powerful datetime function. Let's \"resample\" the dataset and compute the average value for each day. "
      ],
      "metadata": {
        "id": "7rmJMXalKjxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sr_sentiment"
      ],
      "metadata": {
        "id": "NhbTWyPIhQEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for days\n",
        "sr_sentiment.resample('D').mean()"
      ],
      "metadata": {
        "id": "PVAYhTXCL0Z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for months\n",
        "sr_sentiment.resample('D').mean()"
      ],
      "metadata": {
        "id": "LRZTCWiZL1EZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sr_sentiment = sr_sentiment.resample('D').mean()\n",
        "sr_sentiment"
      ],
      "metadata": {
        "id": "7DEidgA2JsTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sr_sentiment.plot(figsize=[15,5], marker='o')"
      ],
      "metadata": {
        "id": "Q6UcT2xhIDsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dealing with missing values"
      ],
      "metadata": {
        "id": "L2I00_-jMweV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What should we do with the missing data? We have several options\n",
        "1. Fill with zeros.\n",
        "2. Fill with most recent known value.\n",
        "3. Fill by interpolating between last known and next known values.\n",
        "\n",
        "What should we do?"
      ],
      "metadata": {
        "id": "0Ro-6v92MCZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since there were not reports on the dates with missing values, one could argue that there was nothing worth reporting about. Thus, zeros would be a neutral way to deal with the missing values, since it denotes neither positive nor negative sentiments."
      ],
      "metadata": {
        "id": "R3Z6MTP1M01M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sr_sentiment = sr_sentiment.fillna(0)"
      ],
      "metadata": {
        "id": "GPgbJEfj4Jyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sr_sentiment.plot(figsize=[15,5], marker='o')"
      ],
      "metadata": {
        "id": "OgVAtbHX4Jyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aggregating over time\n",
        "\n",
        "The data points before are shown as raw data. That each, each day has a value. However, we are interested in trends. For example, the moving average over a week or a month."
      ],
      "metadata": {
        "id": "GRTnhx39NRNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sr_sentiment_week = sr_sentiment.resample('W').mean().diff()\n",
        "sr_sentiment_week.plot(figsize=[15,5], marker='o', title='Rolling weekly average, mean')"
      ],
      "metadata": {
        "id": "xeB-CLPeiNx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sr_sentiment_week = sr_sentiment.rolling(14).mean()\n",
        "sr_sentiment_week.plot(figsize=[15,5], marker='o', title='Rolling weekly average, mean')"
      ],
      "metadata": {
        "id": "9-JdAodmNlmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sr_sentiment_month = sr_sentiment.rolling(30).median()\n",
        "sr_sentiment_month.plot(figsize=[15,5], marker='o', title='Rolling monthly average, mean')"
      ],
      "metadata": {
        "id": "bDZes_0ANzFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sr_sentiment_month = sr_sentiment.rolling(30).std()\n",
        "sr_sentiment_month.plot(figsize=[15,5], marker='o', title='Rolling monthly average, std. dev.')"
      ],
      "metadata": {
        "id": "9PBpaKNY4Jyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Translating word counts into features"
      ],
      "metadata": {
        "id": "Xxpq7OK3GCNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "Z8FHCp_aGDNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "countvec = CountVectorizer()"
      ],
      "metadata": {
        "id": "32YUmqjQDG1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = countvec.fit_transform(df_articles['body_tokenize_nonstop_lemma'].str.join(' '))"
      ],
      "metadata": {
        "id": "peUGdCdNDNRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_features = pd.DataFrame(\n",
        "    X.toarray(), \n",
        "    columns=countvec.get_feature_names_out()\n",
        ")"
      ],
      "metadata": {
        "id": "j1Cz1E-qC0Ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_features"
      ],
      "metadata": {
        "id": "Pj5xf4JLGbHJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}